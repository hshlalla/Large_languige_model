{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f39944c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import os, sys\n",
    "src_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "if src_dir not in sys.path:\n",
    "    sys.path.append(src_dir)\n",
    "from modules.preprocess import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d78c0043",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df=pd.read_excel(\"../../data/250224 model_STD.xlsx\")\n",
    "maker_df=pd.read_excel(\"../../data/250224 maker_STD.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5beae578",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>makerID</th>\n",
       "      <th>makerSTDName</th>\n",
       "      <th>makerAlias</th>\n",
       "      <th>CLEAN_MAKER_NM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MA273101</td>\n",
       "      <td>Bomyeong Hitech</td>\n",
       "      <td>Bomyeong Hitech</td>\n",
       "      <td>BOMYEONGHITECH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MA1538</td>\n",
       "      <td>Nexstar</td>\n",
       "      <td>NEX Star,Nexstar</td>\n",
       "      <td>NEXSTAR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MA298455</td>\n",
       "      <td>NAMYOUNG</td>\n",
       "      <td>NAMYOUNG</td>\n",
       "      <td>NAMYOUNG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MA298457</td>\n",
       "      <td>DHE CO.,LTD</td>\n",
       "      <td>DHE CO.,LTD</td>\n",
       "      <td>DHECO.LTD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MA278365</td>\n",
       "      <td>SAFEX</td>\n",
       "      <td>SAFEX</td>\n",
       "      <td>SAFEX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3778</th>\n",
       "      <td>MA999</td>\n",
       "      <td>ISOTECH</td>\n",
       "      <td>ISOTECH</td>\n",
       "      <td>ISOTECH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3779</th>\n",
       "      <td>MA973</td>\n",
       "      <td>INTERGEN</td>\n",
       "      <td>INTERGEN</td>\n",
       "      <td>INTERGEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3780</th>\n",
       "      <td>MA932</td>\n",
       "      <td>Iglika</td>\n",
       "      <td>Iglika</td>\n",
       "      <td>IGLIKA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3781</th>\n",
       "      <td>MA97</td>\n",
       "      <td>ALPHA AUTOMATION INC</td>\n",
       "      <td>ALPHA,ALPHA AUTOMATION INC</td>\n",
       "      <td>ALPHAAUTOMATIONINC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3782</th>\n",
       "      <td>MA972</td>\n",
       "      <td>InterAction</td>\n",
       "      <td>IA,InterAction,Oyo Electric Co., Ltd</td>\n",
       "      <td>INTERACTION</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3783 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       makerID          makerSTDName                            makerAlias  \\\n",
       "0     MA273101       Bomyeong Hitech                       Bomyeong Hitech   \n",
       "1       MA1538               Nexstar                      NEX Star,Nexstar   \n",
       "2     MA298455              NAMYOUNG                              NAMYOUNG   \n",
       "3     MA298457           DHE CO.,LTD                           DHE CO.,LTD   \n",
       "4     MA278365                 SAFEX                                 SAFEX   \n",
       "...        ...                   ...                                   ...   \n",
       "3778     MA999               ISOTECH                               ISOTECH   \n",
       "3779     MA973              INTERGEN                              INTERGEN   \n",
       "3780     MA932                Iglika                                Iglika   \n",
       "3781      MA97  ALPHA AUTOMATION INC            ALPHA,ALPHA AUTOMATION INC   \n",
       "3782     MA972           InterAction  IA,InterAction,Oyo Electric Co., Ltd   \n",
       "\n",
       "          CLEAN_MAKER_NM  \n",
       "0         BOMYEONGHITECH  \n",
       "1                NEXSTAR  \n",
       "2               NAMYOUNG  \n",
       "3              DHECO.LTD  \n",
       "4                  SAFEX  \n",
       "...                  ...  \n",
       "3778             ISOTECH  \n",
       "3779            INTERGEN  \n",
       "3780              IGLIKA  \n",
       "3781  ALPHAAUTOMATIONINC  \n",
       "3782         INTERACTION  \n",
       "\n",
       "[3783 rows x 4 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maker_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e8fde1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÌäπÏàò ÏòàÏô∏ Ìå®ÌÑ¥ Î≥¥Ï†ï: ÏâºÌëú Ï†úÍ±∞ ÎòêÎäî ÎåÄÏ≤¥\n",
    "maker_df[\"makerAlias\"] = maker_df[\"makerAlias\"].str.lower()\n",
    "maker_df[\"makerAlias\"] = maker_df[\"makerAlias\"].str.replace(\"co.,ltd\", \"co.ltd\", regex=False)\n",
    "maker_df[\"makerAlias\"] = maker_df[\"makerAlias\"].str.replace(\"co., ltd\", \"co.ltd\", regex=False)\n",
    "explode_model_df=alias_info_dataframe(model_df)\n",
    "explode_maker_df=alias_info_dataframe(maker_df,\"makerAlias\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d3a75457",
   "metadata": {},
   "outputs": [],
   "source": [
    "explode_maker_df[\"makerAlias\"]=explode_maker_df[\"makerAlias\"].apply(clean_unknown_to_empty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "07af343c",
   "metadata": {},
   "outputs": [],
   "source": [
    "explode_maker_df.drop_duplicates(subset=[\"makerAlias\",\"makerSTDName\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f655fba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "explode_maker_df.to_csv(\"../../data/test123.csv\", index=False)\n",
    "explode_maker_df=pd.read_csv(\"../../data/test123.csv\")\n",
    "explode_maker_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "51b66ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/workspace/Large_languige_model/.venv/lib/python3.10/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6120' max='6120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6120/6120 04:17, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>7.716000</td>\n",
       "      <td>7.567528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>7.288000</td>\n",
       "      <td>7.573235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>7.152700</td>\n",
       "      <td>7.474590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>6.959700</td>\n",
       "      <td>7.607539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>6.679000</td>\n",
       "      <td>7.524020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>6.591300</td>\n",
       "      <td>7.533133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>6.403400</td>\n",
       "      <td>7.583707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>6.203800</td>\n",
       "      <td>7.508178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>6.218700</td>\n",
       "      <td>7.431211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>6.040800</td>\n",
       "      <td>7.429640</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 1. Îç∞Ïù¥ÌÑ∞ Ï§ÄÎπÑ\n",
    "data = explode_maker_df\n",
    "label_encoder = LabelEncoder()\n",
    "data['label_id'] = label_encoder.fit_transform(data['makerSTDName'])\n",
    "\n",
    "\n",
    "# 2. Ïª§Ïä§ÌÖÄ Dataset Ï†ïÏùò\n",
    "class AliasDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=64):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# 3. ÌïôÏäµ Îç∞Ïù¥ÌÑ∞ Î∂ÑÎ¶¨\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-uncased\")\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    data['makerAlias'].tolist(), data['label_id'].tolist(), test_size=0.1, random_state=42)\n",
    "\n",
    "train_dataset = AliasDataset(train_texts, train_labels, tokenizer)\n",
    "val_dataset = AliasDataset(val_texts, val_labels, tokenizer)\n",
    "\n",
    "# 4. Î™®Îç∏ Î∂àÎü¨Ïò§Í∏∞\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-multilingual-cased\",\n",
    "    num_labels=len(label_encoder.classes_)\n",
    ")\n",
    "\n",
    "# 5. ÌïôÏäµ ÏÑ§Ï†ï\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bert_alias_model\",\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "# 6. ÌïôÏäµ ÏãúÏûë\n",
    "trainer.train()\n",
    "\n",
    "# 7. Ï†ÄÏû•\n",
    "tokenizer.save_pretrained(\"../model/bert_alias_model\")\n",
    "model.save_pretrained(\"../model/bert_alias_model\")\n",
    "\n",
    "# 8. ÏòàÏ∏° Ìï®Ïàò\n",
    "def predict_alias(text):\n",
    "    encoding = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=64)\n",
    "    with torch.no_grad():\n",
    "        output = model(**encoding)\n",
    "    pred = torch.argmax(output.logits, dim=1).item()\n",
    "    return label_encoder.inverse_transform([pred])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2b235a5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Gasonics', 1140)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train Îç∞Ïù¥ÌÑ∞ ÏÖãÏóêÏÑú Ï≤´Î≤àÏß∏Îç∞Ïù¥ÌÑ∞ Î≥¥Ïó¨Ï§ò\n",
    "train_texts[0], train_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a6a7b766",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_alias(text):\n",
    "    encoding = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=64)\n",
    "    \n",
    "    # Î™®Îç∏ ÎîîÎ∞îÏù¥Ïä§ ÌôïÏù∏\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # ÏûÖÎ†• ÌÖêÏÑúÎèÑ Í∞ôÏùÄ ÎîîÎ∞îÏù¥Ïä§Î°ú Ïù¥Îèô\n",
    "    input_ids = encoding[\"input_ids\"].to(device)\n",
    "    attention_mask = encoding[\"attention_mask\"].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    pred = torch.argmax(output.logits, dim=1).item()\n",
    "    return label_encoder.inverse_transform([pred])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73b334c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
